# Messing With Rewards

(Ng, Harada, Russel)

Why do we want to change the reward function for an MDP?

- Easy to solve (and similar to what it would have learned)

## Changing the reward function



## Multiplying by a scalar



## adding a scalar



## reward shaping



## shaping in RL



## Potential based shaping in RL



## State based bonuses



## potential-based shaping



## Q-LEarningn with Potentials



## Summary


